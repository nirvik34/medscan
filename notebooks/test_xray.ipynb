{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da3416c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75d34fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Dataset Paths\n",
    "data_dir = \"../data/chest_xray\"  # root folder\n",
    "\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "val_dir   = os.path.join(data_dir, \"val\")\n",
    "test_dir  = os.path.join(data_dir, \"test\")\n",
    "\n",
    "# %% Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a5e542b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['NORMAL', 'PNEUMONIA']\n",
      "Training samples: 5216\n",
      "Validation samples: 16\n",
      "Test samples: 624\n",
      "Class distribution (train): Counter({1: 3875, 0: 1341})\n"
     ]
    }
   ],
   "source": [
    "# %% Datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "val_dataset   = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "test_dataset  = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "# %% DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"Classes:\", train_dataset.classes)  # ['NORMAL', 'PNEUMONIA']\n",
    "print(\"Training samples:\", len(train_dataset))\n",
    "print(\"Validation samples:\", len(val_dataset))\n",
    "print(\"Test samples:\", len(test_dataset))\n",
    "\n",
    "print(\"Class distribution (train):\", Counter(train_dataset.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "197cc2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Model Definition\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):  # 2 classes: NORMAL, PNEUMONIA\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*32*32, 128), nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = SimpleCNN(num_classes=len(train_dataset.classes))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a8ff426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.1132, Train Acc: 0.9563\n",
      "Epoch 2, Train Loss: 0.0693, Train Acc: 0.9753\n",
      "Epoch 3, Train Loss: 0.0503, Train Acc: 0.9799\n",
      "Epoch 4, Train Loss: 0.0284, Train Acc: 0.9895\n",
      "Epoch 5, Train Loss: 0.0234, Train Acc: 0.9921\n",
      "Epoch 6, Train Loss: 0.0076, Train Acc: 0.9973\n",
      "Epoch 7, Train Loss: 0.0176, Train Acc: 0.9946\n",
      "Epoch 8, Train Loss: 0.0044, Train Acc: 0.9985\n",
      "Epoch 9, Train Loss: 0.0091, Train Acc: 0.9964\n",
      "Epoch 10, Train Loss: 0.0076, Train Acc: 0.9971\n"
     ]
    }
   ],
   "source": [
    "# %% Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    for imgs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {total_loss/len(train_loader):.4f}, Train Acc: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb6c4fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6875\n"
     ]
    }
   ],
   "source": [
    "# %% Validation\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "val_acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c051b05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../backend/models/cnn_chestxray.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Save Model\n",
    "model_path = \"../backend/models/cnn_chestxray.pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'num_classes': len(train_dataset.classes)\n",
    "}, model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e31c73eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and ready for inference.\n"
     ]
    }
   ],
   "source": [
    "# %% Load Model Later\n",
    "checkpoint = torch.load(model_path)\n",
    "model = SimpleCNN(num_classes=checkpoint['num_classes'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model.eval()\n",
    "print(\"Model loaded and ready for inference.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ai-tumor",
   "language": "python",
   "name": "venv-ai-tumor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
