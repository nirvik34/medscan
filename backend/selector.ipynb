{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29b54784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All models loaded and ready for inference\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Correct architecture (must match training)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            # nn.AdaptiveAvgPool2d((4, 4)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*32*32, 128), nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Load CT\n",
    "checkpoint = torch.load(\"models/ct_model.pth\", map_location=\"cpu\")\n",
    "ct_model = SimpleCNN(num_classes=checkpoint[\"num_classes\"])\n",
    "ct_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "ct_model.eval()\n",
    "\n",
    "# Load X-ray\n",
    "checkpoint = torch.load(\"models/cnn_chestxray.pth\", map_location=\"cpu\")\n",
    "xray_model = SimpleCNN(num_classes=checkpoint[\"num_classes\"])\n",
    "xray_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "xray_model.eval()\n",
    "\n",
    "# Load Ultrasound\n",
    "checkpoint = torch.load(\"models/ultrasound_model.pth\", map_location=\"cpu\")\n",
    "ultrasound_model = SimpleCNN(num_classes=checkpoint[\"num_classes\"])\n",
    "ultrasound_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "ultrasound_model.eval()\n",
    "\n",
    "print(\"✅ All models loaded and ready for inference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bda7030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Image transforms\n",
    "# ------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # RGB mean/std\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8f11c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(modality):\n",
    "    if modality.lower() == \"ct\":\n",
    "        return ct_model\n",
    "    elif modality.lower() == \"xray\":\n",
    "        return xray_model\n",
    "    elif modality.lower() == \"ultrasound\":\n",
    "        return ultrasound_model\n",
    "    else:\n",
    "        raise ValueError(\"Unknown modality. Please specify CT, X-ray, or Ultrasound.\")\n",
    "\n",
    "def predict(image_path, modality):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # add batch dimension\n",
    "\n",
    "    if modality.lower() == \"ct\":\n",
    "        model = ct_model\n",
    "    elif modality.lower() == \"xray\":\n",
    "        model = xray_model\n",
    "    elif modality.lower() == \"ultrasound\":\n",
    "        model = ultrasound_model\n",
    "    else:\n",
    "        raise ValueError(\"Invalid modality! Choose from: ct, xray, ultrasound\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        confidence, pred = torch.max(probs, 1)\n",
    "\n",
    "    return pred.item(), confidence.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b73ce21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modality = input(\"Enter modality (ct / xray / ultrasound): \")\n",
    "image_path = input(\"Enter image path: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b9fa8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Normal\n",
      "Confidence: 0.9928\n"
     ]
    }
   ],
   "source": [
    "pred, confidence = predict(image_path, modality)\n",
    "print(f\"Prediction: {'Anomaly' if pred == 1 else 'Normal'}\")\n",
    "print(f\"Confidence: {confidence:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bbc9023d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ct_model output shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# Quick smoke test: dummy forward pass to validate shapes\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "    dummy = torch.randn(1, 3, 128, 128)  # batch 1, RGB, 128x128\n",
    "    out = ct_model(dummy)\n",
    "    print('ct_model output shape:', out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2380a73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "def evaluate(model, dataloader, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            # handle one-hot labels -> convert to class indices\n",
    "            if labels.dim() > 1 and labels.size(1) > 1:\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            # handle binary (single-logit) vs multiclass outputs\n",
    "            if outputs.dim() == 1 or (outputs.dim() == 2 and outputs.size(1) == 1):\n",
    "                probs = torch.sigmoid(outputs.view(-1))\n",
    "                preds = (probs > 0.5).long()\n",
    "            else:\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                preds = torch.argmax(probs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    print(f\"Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(all_labels, all_preds, digits=4))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "    return all_preds, all_labels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ai-tumor",
   "language": "python",
   "name": "venv-ai-tumor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
